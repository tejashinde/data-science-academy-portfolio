CatBoost is a fast, scalable, high performance gradient boosting on decision trees library. Used for ranking, classification, regression and other ML tasks.
- Reduce time spent on parameter tuning, because CatBoost provides great results with default parameters
- Improve your training results with CatBoost that allows you to use non-numeric factors, instead of having to pre-process your data or spend time and effort turning it to numbers.
- Train your model on a fast implementation of gradient-boosting algorithm for GPU
- Apply your trained model quickly and efficiently even to latency-critical tasks using CatBoost's model applier


How training is performed ?

The goal of training is to select the model , depending on a set of features , that best solves the given problem (regression, classification, or multiclassification) for any
input object. This model is found by using a training dataset, which is a set of objects with known features and label values. Accuracy is checked on the validation dataset,
which has data in the same format as in the training dataset, but it is only used for evaluating the quality of training (it is not used for training).

CatBoost is based on gradient boosted decision trees. During training, a set of decision trees is built consecutively. Each successive tree is built with reduced loss 
compared to the previous trees.

The number of trees is controlled by the starting parameters. To prevent overfitting, use the overfitting detector. When it is triggered, trees stop being built.
Overfitting detector :
If overfitting occurs, CatBoost can stop the training earlier than the training parameters dictate. For example, it can be stopped before the specified number of trees are built. 
This option is set in the starting parameters.


